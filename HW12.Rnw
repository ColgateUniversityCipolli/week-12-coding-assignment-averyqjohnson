\documentclass{article}
\usepackage[margin=1.0in]{geometry} % To set margins
\usepackage{amsmath}  % This allows me to use the align functionality.
                      % If you find yourself trying to replicate
                      % something you found online, ensure you're
                      % loading the necessary packages!
\usepackage{amsfonts} % Math font
\usepackage{fancyvrb}
\usepackage{hyperref} % For including hyperlinks
\usepackage[shortlabels]{enumitem}% For enumerated lists with labels specified
                                  % We had to run tlmgr_install("enumitem") in R
\usepackage{float}    % For telling R where to put a table/figure
\usepackage{natbib}        %For the bibliography
\bibliographystyle{apalike}%For the bibliography

\begin{document}
<<echo=F, message=F, warning=F>>=
library(tidyverse)
@

\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item A group of researchers is running an experiment over the course of 30 months, 
with a single observation collected at the end of each month. Let $X_1, ..., X_{30}$
denote the observations for each month. From prior studies, the researchers know that
\[X_i \sim f_X(x),\]
but the mean $\mu_X$ is unknown, and they wish to conduct the following test
\begin{align*}
H_0&: \mu_X = 0\\
H_a&: \mu_X > 0.
\end{align*}
At month $k$, they have accumulated data $X_1, ..., X_k$ and they have the 
$t$-statistic
\[T_k = \frac{\bar{X} - 0}{S_k/\sqrt{n}}.\]
The initial plan was to test the hypotheses after all data was collected (at the 
end of month 30), at level $\alpha=0.05$. However, conducting the experiment is 
expensive, so the researchers want to ``peek" at the data at the end of month 20 
to see if they can stop it early. That is, the researchers propose to check 
whether $t_{20}$ provides statistically discernible support for the alternative. 
If it does, they will stop the experiment early and report support for the 
researcher's alternative hypothesis. If it does not, they will continue to month 
30 and test whether $t_{30}$ provides statistically discernible support for the
alternative.

\begin{enumerate}
  \item What values of $t_{20}$ provide statistically discernible support for the
  alternative hypothesis?
  
  <<size="scriptsize">>=
  # (a) t-val for statistically discernible support for t20
  # gives t val at 95th percentile 
  (val_t20 <- qt(0.95, df=19))
@

To determine whether the researchers should stop the experiment at month $20$,
we calculated the critical value of the one-sided t-test at the 5\% significance
level with 19 degrees of freedom (since $n=20$). This gave a critical value of
$1.729$, so if $t_{20} > 1.729$, the researchers would reject $H_{0}$ and stop early.
  
  \item What values of $t_{30}$ provide statistically discernible support for the
  alternative hypothesis?
  
   <<size="scriptsize">>=
  # (b) t-val for statistically discernible support for t30
  # gives t val at 95th percentile 
  (val_t30 <- qt(0.95, 29))
  @
  
If the experiment continues to month 30, the researchers use the full sample to compute a new test statistic and compare it to the critical value for a t-test with 29 degrees of freedom (since $n=30$). This gave a critical value of $1.699$,
so if $t_{30} > 1.699$, they would reject $H_{0}$ at that point.


  \item Suppose $f_X(x)$ is a Laplace distribution with $a=0$ and $b=4.0$.
  Conduct a simulation study to assess the Type I error rate of this approach.\\
  \textbf{Note:} You can use the \texttt{rlaplace()} function from the \texttt{VGAM}
  package for \texttt{R} \citep{VGAM}.
  
  <<size="scriptsize">>=
  # (c) simulation to estimate type 1 error

  library(VGAM)
  simulations <- 10000
  alpha <- 0.05
  
  type_1_error_count <- 0
  
  for (i in 1:simulations) {
    # generate data from Laplace
    data <- rlaplace(30, location=0, scale=4)
    
    # perform t test
    t20_result <- t.test(data[1:20], mu=0)
    t20 <- t20_result$statistic
    
    t30_result <- t.test(data, mu=0)
    t30 <- t30_result$statistic
    
    # check if we would reject the null at month 20
    # do the t-stats exceed the corresponding critical values?
      # if they do, reject the null
    if (t20 > val_t20) {
      type_1_error_count <- type_1_error_count + 1
      # if this doesnt reject, then we check t30
    } else if (t30 > val_t30) {
      type_1_error_count <- type_1_error_count + 1
    }
  }
  
  # estimate type 1 error rate
  (type_1_error_rate <- type_1_error_count / simulations)
  @
  
To examine the impact of this strategy, we simulated 10,000 experiments under
the null hypothesis, where data were drawn from a Laplace distribution with mean zero and scale 4. In each simulation, we computed $t_{20}$. If it was significant, we stopped and counted it as rejection. Otherwise, we computed $t_{30}$ and checked again. This procedure gave a type one error rate of approximately 0.073, or 7.3\%, which is noticeably higher than the intended 5\%. This confirms that checking twice without adjusting for it increases the overall chance of making a
Type I error.
  
  \item \textbf{Optional Challenge:} Can you find a value of $\alpha<0.05$ that yields a 
  Type I error rate of 0.05?
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item Perform a simulation study to assess the robustness of the $T$ test. 
  Specifically, generate samples of size $n=15$ from the Beta(10,2), Beta(2,10), 
  and Beta(10,10) distributions and conduct the following hypothesis tests against 
  the actual mean for each case (e.g., $\frac{10}{10+2}$, $\frac{2}{10+2}$, and 
  $\frac{10}{10+10}$). 
  \begin{enumerate}
    \item What proportion of the time do we make an error of Type I for a
    left-tailed test?
    
      <<size="scriptsize">>=
    ################################################################################
  # Question 2
  ################################################################################
  
  simulations <- 10000
  alpha <- 0.05
  n <- 15
  
  true_means <- c(10 / (10+2), 2 / (2+10), 10/(10+10))
  
  # initialize counters
  type_1_error_left <- c(0,0,0)
  type_1_error_right<- c(0,0,0)
  type_1_error_two_tailed <- c(0,0,0)
  
  for (i in 1:simulations){
    # generate samples from Beta distributions
    data_beta1 <- rbeta(n, 10, 2)
    data_beta2 <- rbeta(n, 2, 10)
    data_beta3 <- rbeta(n, 10, 10)
    
    # conduct left tailed t test
    t_left_1 <- t.test(data_beta1, mu=true_means[1], alternative="less")
    t_left_2 <- t.test(data_beta2, mu=true_means[2], alternative="less")
    t_left_3 <- t.test(data_beta3, mu=true_means[3], alternative="less")
    
    # conduct right tailed t test
    t_right_1 <- t.test(data_beta1, mu=true_means[1], alternative="greater")
    t_right_2 <- t.test(data_beta2, mu=true_means[2], alternative="greater")
    t_right_3 <- t.test(data_beta3, mu=true_means[3], alternative="greater")
    
    # conduct two tailed t test
    t_two_1 <- t.test(data_beta1, mu=true_means[1], alternative="two.sided")
    t_two_2 <- t.test(data_beta2, mu=true_means[2], alternative="two.sided")
    t_two_3 <- t.test(data_beta3, mu=true_means[3], alternative="two.sided")
    
    # count type one errors
    # Count Type I errors
    type_1_error_left[1] <- type_1_error_left[1] + (t_left_1$p.value < alpha)
    type_1_error_left[2] <- type_1_error_left[2] + (t_left_2$p.value < alpha)
    type_1_error_left[3] <- type_1_error_left[3] + (t_left_3$p.value < alpha)
    
    type_1_error_right[1] <- type_1_error_right[1] + (t_right_1$p.value < alpha)
    type_1_error_right[2] <- type_1_error_right[2] + (t_right_2$p.value < alpha)
    type_1_error_right[3] <- type_1_error_right[3] + (t_right_3$p.value < alpha)
    
    type_1_error_two_tailed[1] <- type_1_error_two_tailed[1] + (t_two_1$p.value < alpha)
    type_1_error_two_tailed[2] <- type_1_error_two_tailed[2] + (t_two_2$p.value < alpha)
    type_1_error_two_tailed[3] <- type_1_error_two_tailed[3] + (t_two_3$p.value < alpha)
  }
  
  
  type_1_error_rate_left <- type_1_error_left / simulations
  
  # (a) proportion of time we make a Type 1 error for left-tailed
  (type_1_error_rate_left <- type_1_error_left / simulations)
  
  @
  When using a left-tailed t-test, the Type 1 error rate was around 3\% for Beta(10,2), 8\% for Beta(2,10), and 5\% for Beta(10,10). These results suggest that skewness can substantially influence error rates, with the right-skewed distribution (Beta(2,10)) producing an inflated rate of false positives when using a left-tailed test.
  
    \item What proportion of the time do we make an error of Type I for a
    right-tailed test?
    
<<size="scriptsize">>=
  # (b) proportion of time we make a Type 1 error for right-tailed
(type_1_error_rate_right <- type_1_error_right / simulations)
  @
  
  For right-tailed tests, Type 1 error occurred about 8\% of the time for Beta(10,2), 3\% of the time for Beta(2,10), and 5\% of the time for Beta(10,10). The left-skewed distribution Beta(10,2) distribution shows an elevated error rate, whereas the right-skewed Beta(2,10) suppresses it. Again, the symmetric distribution comes closest to the typical 5\%.

    \item What proportion of the time do we make an error of Type I for a
    two-tailed test?
    
    <<size="scriptsize">>=
  # (c) proportion of time we make a Type 1 error for two-tailed
(type_1_error_rate_two_tailed <- type_1_error_two_tailed / simulations)
  @
    
    Two-tailed tests produced more stable results, with error rates of 6\% for Beta(10,2), 6\% for Beta(2,10), and 5\% for Beta(10,10). The symmetric distribution aligns most closely with the expected Type 1 error, while skewed distributions only deviate slightly.
    
    \item How does skewness of the underlying population distribution effect
    Type I error across the test types?
    
<<size="scriptsize">>=
 # (d) how does skewness effect Type 1 error
error_comparison <- data.frame(
  distribution = c("Beta(10,2)", "Beta(2,10)", "Beta(10,10)"),
  left_tailed = type_1_error_rate_left,
  right_tailed = type_1_error_rate_right,
  two_tailed = type_1_error_rate_two_tailed
) 
@
    
<<echo=FALSE, eval=TRUE, results="asis", message=FALSE, warning=FALSE, size='scriptsize'>>=
  
error_comparison <- data.frame(
  distribution = c("Beta(10,2)", "Beta(2,10)", "Beta(10,10)"),
  left_tailed = type_1_error_rate_left,
  right_tailed = type_1_error_rate_right,
  two_tailed = type_1_error_rate_two_tailed
)

library(xtable)
latex_table <- xtable(error_comparison, 
                      caption="Type 1 Error Comparison",
                      label="errorcomparison")
  @
  
<<echo=FALSE, eval=TRUE, results="asis">>=
# placement="H" places table [H]ere, just like plot
# include.rownames=FALSE doesn’t print the row numbers in this example
print(latex_table,
table.placement = "H", include.rownames=FALSE, size = "small")
@

Skewness affects Type 1 error rates asymmetrically depending on the direction of the test. Right-skewed distributions like Beta(2,10) inflate the error rate for left-tailed tests and reduce it for right-tailed tests. Left-skewed distributions like Beta(10,2) show the opposite pattern. Two-tailed tests are less sensitive to skew, though they still deviate slightly from the nominal rate when the data are not symmetric. These patterns suggest that the t-test is less robust under skewed coniditions, especially for one-sided hypotheses with small samples.

  \end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{enumerate}
\bibliography{bibliography}
\end{document}
